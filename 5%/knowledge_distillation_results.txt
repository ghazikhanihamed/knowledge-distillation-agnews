==================================================
Knowledge Distillation Results
==================================================

Model Parameter Counts:
Teacher (BERT): 109,485,316 parameters
Student (DistilBERT): 66,956,548 parameters
Parameter Reduction: 38.84%

Performance Metrics:
Model                                    Accuracy   F1 Score  
------------------------------------------------------------
Teacher (BERT)                           0.9263    0.9266
Baseline Student                         0.8868    0.8871
Response-based KD (T=0.5, Î±=0.5, lr=5e-6) 0.8921    0.8922


Methodology Notes:
1. Response-based Knowledge Distillation: Uses soft targets from teacher logits with temperature scaling.
2. Feature-based Knowledge Distillation: Aligns intermediate representations between teacher and student.
3. Hyperparameter optimization was performed for both methods.


File generated on: 2025-05-16 07:51:45