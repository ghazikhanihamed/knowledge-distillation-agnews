 Results Summary - 2025-05-17 19:52:12
======================================================================
Settings: {'name': 'run2', 'lr_enc': 3e-05, 'lr_dec': 2e-05, 'temp': 5.0, 'alpha': 0.3, 'epochs_enc': 2, 'epochs_dec': 2, 'batch_size': 64}
120000 training examples, 7600 test examples
Classes: ['World', 'Sports', 'Business', 'Sci/Tech']
Teacher model: fabriceyhc/bert-base-uncased-ag_news
Student model: distilbert-base-uncased

EXPERIMENT 1: Training baseline student model...
Baseline student trained in 1893.3 seconds
Accuracy: 0.9429, F1: 0.9429

EXPERIMENT 2: Training student with knowledge distillation...
Distilled student trained in 3039.0 seconds
Accuracy: 0.9403, F1: 0.9403

Evaluating teacher model...
Teacher accuracy: 0.9336, F1: 0.9334

EXPERIMENT 3: Distillation with Decoder-Only Student...
Pre-fine-tuning decoder-only student...
Pre-fine-tuned decoder accuracy: 0.9341, F1: 0.9340
Training decoder-only student with knowledge distillation...
Decoder-only student trained in 3500.9 seconds
Accuracy: 0.9038, F1: 0.9045

MODEL COMPARISON
======================================================================
Model                Accuracy   F1 Score   Parameters   Time (s)  
-------------------- ---------- ---------- ------------ ----------
BERT (Teacher)       0.9336     0.9334     109,485,316  N/A       
DistilBERT           0.9429     0.9429     66,956,548  1893.3
DistilBERT + KD      0.9403     0.9403     66,956,548  3039.0
DistilGPT2 + KD      0.9038     0.9045     81,915,648  3500.9
======================================================================

IMPROVEMENT FROM KNOWLEDGE DISTILLATION:
Accuracy: +-0.0026 absolute (+-0.26%)
F1 Score: +-0.0026 absolute (+-0.26%)
Compression: 1.6x smaller than teacher
Performance: 100.7% of teacher accuracy with only 61.2% of parameters
 